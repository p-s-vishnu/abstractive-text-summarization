{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abstractive-text-summ.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p-s-vishnu/abstractive-text-summarization/blob/master/abstractive-text-summ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_FILznqy7Lg",
        "colab_type": "text"
      },
      "source": [
        "Abstractive Text Summarization\n",
        "===\n",
        "This notebook is an in-progress implementation/experiment of the [Abstractive Text Summarization using Sequence-to-sequence RNNs and\n",
        "Beyond](https://arxiv.org/abs/1602.06023) paper.\n",
        "\n",
        "Current Features\n",
        "---\n",
        "* model architecture supports LSTM & GRU (biLSTM-to-uniLSTM or biGRU-to-uniGRU)\n",
        "* implements batch data processing \n",
        "* implements attention mechanism ([Bahdanau et al.](https://arxiv.org/abs/1409.0473) & [Luong et al.(global dot)](https://arxiv.org/abs/1508.04025))\n",
        "* implements [scheduled sampling (teacher forcing)](https://arxiv.org/abs/1506.03099)\n",
        "* implements [tied embeddings](https://arxiv.org/pdf/1608.05859.pdf)\n",
        "* initializes encoder-decoder with pretrained vectors (glove.6B.200d)\n",
        "* implements custom training callbacks (tensorboard visualization for PyTorch, save best model & log checkpoint)\n",
        "* implements attention plots\n",
        "\n",
        "\n",
        "To-Do\n",
        "---\n",
        "* Implement additional linguistic features embeddings  \n",
        "* Implement generator-pointer switch and replace unknown words by selecting source token with the highest attention score.\n",
        "* Implement large vocabulary trick \n",
        "* Implement sentence level attention \n",
        "* Implement beam search during inference\n",
        "* implement rouge evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdpZPOry7Lj",
        "colab_type": "text"
      },
      "source": [
        "Requirements\n",
        "---\n",
        "\n",
        "1. Create conda environment \n",
        "\n",
        "`conda env create -f environment.yml`  --gpu\n",
        "\n",
        "`conda env create -f environment-cpu.yml`  --cpu\n",
        "\n",
        "2. Install dependencies (PyTorch, Fastai, TorchText, Tensorboard etc) via:\n",
        "\n",
        "`pip install -r requirements.txt`\n",
        "\n",
        "3. Download `spacy` english module\n",
        "\n",
        "`python -m spacy download en`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZQ0mP_iy7Lm",
        "colab_type": "text"
      },
      "source": [
        "Dataset\n",
        "--\n",
        "\n",
        "The dataset used is a subset of the gigaword dataset and can be found [here](https://drive.google.com/file/d/0B6N7tANPyVeBNmlSX19Ld2xDU1E/view?usp=sharing).\n",
        "\n",
        "It contains 3,803,955 parallel source & target examples for training and 189,649 examples for validation.\n",
        "\n",
        "After downloading, we create article-title pairs, save in tabular datset format (.csv) and extract a sample subset (80,000 for training & 20,000 for validation). This data preparation can be found [here](/data-preparation.ipynb).\n",
        "\n",
        "An example article-title pair looks like this:\n",
        "\n",
        "`article: the algerian cabinet chaired by president abdelaziz bouteflika on sunday adopted the #### finance bill predicated on an oil price of ## dollars a barrel and a growth rate of #.# percent , it was announced here .`\n",
        "\n",
        "`title: algeria adopts #### finance bill with oil put at ## dollars a barrel`\n",
        "\n",
        "\n",
        "Training on the complete dataset (3M) would take a really long time. So in order to train and experiment faster we use our sample subset of 80,000 in this tutorial. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzU_Y3J4y7Lo",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giQMjV6j7q_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "4b94823a-56bc-4df6-84f0-100671233933"
      },
      "source": [
        "!git clone https://github.com/alesee/abstractive-text-summarization"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'abstractive-text-summarization'...\n",
            "remote: Enumerating objects: 169, done.\u001b[K\n",
            "remote: Total 169 (delta 0), reused 0 (delta 0), pack-reused 169\u001b[K\n",
            "Receiving objects: 100% (169/169), 327.58 MiB | 40.89 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n",
            "Checking out files: 100% (36/36), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od5qiU_77vWm",
        "colab_type": "code",
        "outputId": "9b634e3e-aa66-4a8a-f85a-0ad6a2d05a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd abstractive-text-summarization/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/abstractive-text-summarization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUPmpGGt72Rw",
        "colab_type": "code",
        "outputId": "ac1d44e6-42c2-4215-fe87-4edd161606ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " abstractive-text-summ.ipynb   environment.yml                README.md\n",
            " \u001b[0m\u001b[01;34mdata\u001b[0m/                         \u001b[01;34mimgs\u001b[0m/                          requirements.txt\n",
            " data-preparation.ipynb        launch-tensorboard_viz.ipynb\n",
            " environment-cpu.yml          \u001b[01;34m'py scripts'\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35AVhk9K8DKT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1094
        },
        "outputId": "d1b1b233-72cc-42cc-a724-b4dc2fda4b98"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/fastai/fastai.git (from -r requirements.txt (line 1))\n",
            "  Cloning https://github.com/fastai/fastai.git to /tmp/pip-req-build-n69p0qt_\n",
            "  Running command git clone -q https://github.com/fastai/fastai.git /tmp/pip-req-build-n69p0qt_\n",
            "Collecting git+https://github.com/lanpa/tensorboard-pytorch (from -r requirements.txt (line 2))\n",
            "  Cloning https://github.com/lanpa/tensorboard-pytorch to /tmp/pip-req-build-gk6qylfe\n",
            "  Running command git clone -q https://github.com/lanpa/tensorboard-pytorch /tmp/pip-req-build-gk6qylfe\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (1.2.1)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (0.1.21)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (4.6.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (2.6.9)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (1.16.3)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (7.352.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (0.24.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (19.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (4.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (2.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (1.2.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (3.6.6)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (2.0.18)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (0.2.2.post3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.53.dev0->-r requirements.txt (line 1)) (0.6)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.6+0bf6c07->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.6+0bf6c07->-r requirements.txt (line 2)) (1.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (0.46)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (1.24.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (0.2.9)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (1.0.2)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (2018.1.10)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (6.12.1)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (1.35)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.6+0bf6c07->-r requirements.txt (line 2)) (41.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (4.28.1)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (0.4.3.2)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (0.9.0.1)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (1.10.11)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (0.5.6)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai==1.0.53.dev0->-r requirements.txt (line 1)) (0.9.0)\n",
            "Building wheels for collected packages: fastai, tensorboardX\n",
            "  Building wheel for fastai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6nr6fejv/wheels/cf/46/39/b2d08762125ed2376861976ab2c4ac30c029b86e375735d9b8\n",
            "  Building wheel for tensorboardX (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6nr6fejv/wheels/84/b5/d9/7fdb3057d8ab54436a7f143a8f33ee45e7163482e14a805b26\n",
            "Successfully built fastai tensorboardX\n",
            "Installing collected packages: fastai, tensorboardX\n",
            "  Found existing installation: fastai 1.0.52\n",
            "    Uninstalling fastai-1.0.52:\n",
            "      Successfully uninstalled fastai-1.0.52\n",
            "Successfully installed fastai-1.0.53.dev0 tensorboardX-1.6+0bf6c07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJLcX7o5y7Lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoQHNJyay7Ls",
        "colab_type": "code",
        "outputId": "32c4fbe2-71f1-40c0-a93f-3fdb87b7788d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#declare the directory path to dataset  \n",
        "DATA_PATH = 'data/'\n",
        "SAMPLE_DATA_PATH = f'{DATA_PATH}sample_data/'\n",
        "PROCESSED_DATA_PATH = f'{DATA_PATH}processed_data/'\n",
        "\n",
        "#Enable GPU training \n",
        "import torch\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "print('USE_GPU={}'.format(USE_GPU))\n",
        "if USE_GPU:\n",
        "    print('current_device={}'.format(torch.cuda.current_device()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USE_GPU=True\n",
            "current_device=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "ovyzBSSSy7L0",
        "colab_type": "text"
      },
      "source": [
        "## 1. Process dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "RfoSRtJDy7L1",
        "colab_type": "text"
      },
      "source": [
        "In order to train, we performed common processing steps on the dataset such as:\n",
        "\n",
        "* Loading dataset\n",
        "* Preprocessing dataset (tokenizing, appending begining-of-sentence and end-of-sentence tokens, truncating, etc)\n",
        "* Building a vocabulary\n",
        "* Creating dataset iterators\n",
        "* Batching, padding, and numericalizing. \n",
        "\n",
        "To process the dataset, we use the [torchtext](https://github.com/pytorch/text) library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "dnED98kqy7L2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import data, vocab from torchtext \n",
        "from torchtext import data, vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "kQaqPg5uy7L5",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Load & define preprocessing pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "KA8JqtZsy7L6",
        "colab_type": "text"
      },
      "source": [
        "To pre-process our data, we declare a `Field` class, and pass additional pre-processing arguments ( ex. tokenize with the `spacy` tokenizer, `lower` and append an `end-of-sentence` token to every example )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "wGHoomkiy7L7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = data.get_tokenizer('spacy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "Nlv_mbmqy7L-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize=tokenizer, lower=True, eos_token='_eos_')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "veauLMbIy7MB",
        "colab_type": "text"
      },
      "source": [
        "Next, we load our training & validation tabular dataset using `data.TabularDataset.splits` which applies the defined preprocessing pipeline and returns their respective `Dataset` objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "aSTcvzh_y7MC",
        "colab_type": "code",
        "outputId": "d66ac65e-dce7-4eb0-bbf8-9dd6667350e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "trn_data_fields = [(\"source\", TEXT),\n",
        "                   (\"target\", TEXT)]\n",
        "\n",
        "trn, vld = data.TabularDataset.splits(path=f'{SAMPLE_DATA_PATH}',\n",
        "                                     train='train_ds.csv', validation='valid_ds.csv',\n",
        "                                     format='csv', skip_header=True, fields=trn_data_fields)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 39.6 s, sys: 269 ms, total: 39.9 s\n",
            "Wall time: 39.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "YF3z0TfJy7MH",
        "colab_type": "code",
        "outputId": "14cc052c-ca1f-4dbf-b756-52618dacc8f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# a sample of the preprocessed data\n",
        "print(trn[0].source, trn[0].target)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['jason', 'blake', 'of', 'the', 'islanders', 'will', 'miss', 'the', 'rest', 'of', 'the', 'season', 'so', 'he', 'can', 'be', 'with', 'his', 'wife', ',', 'who', 'has', 'thyroid', 'cancer', 'and', 'is', 'to', 'give', 'birth', 'april', '#', '.'] ['blake', 'missing', 'rest', 'of', 'season']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "q5lsd5Kcy7ML",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. Build vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "dmvXvkiLy7MM",
        "colab_type": "text"
      },
      "source": [
        "Building a vocabulary simply means mapping each unique token in the corpus to an integer value, and storing as a dictionary ex `{'the': 2, 'brown':3, 'fox':4}`. \n",
        "\n",
        "In addition to building a vocabulary, we also use torchtext to load an embedding matrix for each token using the `glove.6B.200d` pretrained vector.\n",
        "\n",
        "We pass to `TEXT.build_vocab` our training dataset object `trn` and also the name of the pretrained vector we would like to use `glove.6B.200d`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "x7SP3CMVy7MM",
        "colab_type": "code",
        "outputId": "15db71f1-0b34-4056-bb70-e7ae30ee59ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%%time\n",
        "pre_trained_vector_type = 'glove.6B.200d' \n",
        "TEXT.build_vocab(trn, vectors=pre_trained_vector_type )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [04:03, 3.54MB/s]                           \n",
            "100%|█████████▉| 399413/400000 [00:28<00:00, 15193.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 51.6 s, sys: 7.79 s, total: 59.4 s\n",
            "Wall time: 4min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "M1b3xy23y7MS",
        "colab_type": "code",
        "outputId": "77d2dcab-ef5b-445e-ad82-e7d273d38b93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "#10 most frequent words in the vocab\n",
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('#', 152957),\n",
              " ('the', 130459),\n",
              " ('.', 105054),\n",
              " (',', 85497),\n",
              " ('to', 83508),\n",
              " ('in', 78169),\n",
              " ('of', 77424),\n",
              " ('a', 71025),\n",
              " ('on', 43536),\n",
              " ('and', 42555)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "DXdFulguy7Ma",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Create dataset iterator, batch, pad, and numericalize. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "3OpNVVdBy7Mb",
        "colab_type": "text"
      },
      "source": [
        "Next, create a training & validation iterator object, numericalize (turn text to tensors), batch examples of similar lengths together, randomly shuffle the data and pad tensors using `data.BucketIterator.splits`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "MTfWtW6ky7Mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "pdGelYrTy7Me",
        "colab_type": "code",
        "outputId": "941f8ab7-ccc6-49b5-a930-65f3c8cf143d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train_iter, val_iter = data.BucketIterator.splits(\n",
        "                        (trn, vld), batch_sizes=(batch_size, int(batch_size*1.6)),\n",
        "                        device=(0 if USE_GPU else -1), \n",
        "                        sort_key=lambda x: len(x.source),\n",
        "                        shuffle=True, sort_within_batch=False, repeat=False)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "3IjKoMKFy7Mh",
        "colab_type": "text"
      },
      "source": [
        "Next, we stick each article-title batch pair tensor into a tuple (article, title). To do this we create a custom helper class `BatchTuple`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "RhBRTlvry7Mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchTuple():\n",
        "    def __init__(self, dataset, x_var, y_var):\n",
        "        self.dataset, self.x_var, self.y_var = dataset, x_var, y_var\n",
        "        \n",
        "    def __iter__(self):\n",
        "        for batch in self.dataset:\n",
        "            x = getattr(batch, self.x_var) \n",
        "            y = getattr(batch, self.y_var)                 \n",
        "            yield (x, y)\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "ogF43Zz_y7Mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#returns tuple of article-title pair tensors\n",
        "train_iter_tuple = BatchTuple(train_iter, \"source\", \"target\")\n",
        "val_iter_tuple = BatchTuple(val_iter, \"source\", \"target\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "B_FI9Afsy7Mt",
        "colab_type": "code",
        "outputId": "26417942-20b6-4602-f1f6-e6fc1b9d044d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1962
        }
      },
      "source": [
        "#an example of a batched and padded article-title tensor pair\n",
        "next(iter(train_iter_tuple))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[    8,  2694,     4,  ...,  7436, 14906,  2345],\n",
              "         [    4,  2777,  3035,  ...,    37,    26,   113],\n",
              "         [  172,   656, 12709,  ...,   957,     4,    55],\n",
              "         ...,\n",
              "         [    1,     1,     1,  ...,     1,     1,     1],\n",
              "         [    1,     1,     1,  ...,     1,     1,     1],\n",
              "         [    1,     1,     1,  ...,     1,     1,     1]]),\n",
              " tensor([[   15,  1141,  3035, 10456,  3405,  8644,  1595,  2176, 11804,  2121,\n",
              "           2249,  5694, 16630,   135,    71,   991,   522, 36661,    48,    18,\n",
              "           8114,   204,   139,  5949,   173,   201,   447,   726,   738,    39,\n",
              "            452,   593,   452,   714, 34551,  9030,  1641,   471, 18504,    42,\n",
              "            533, 10188,  2641,    18, 10049,   434,   403,  8305,  4235,   340,\n",
              "            426,  2251,   447,  1226,  1595,  3915, 11194,   192,  3005,   729,\n",
              "            121, 11881,  2575,  2345],\n",
              "         [    4,  1202, 12709,  1583, 36917,   737,    13,  7486, 28230,   494,\n",
              "           5566,     9,   737,   976,   182,    13,   224,  7735, 31151,     4,\n",
              "            984,  5429,   248,    73,    13, 12659,   224,   129,   704,  2488,\n",
              "            984,    13,    13,   732,     8,  3343,  1297,   143,  7415,   324,\n",
              "            428,   182,   381,     3, 17030,  4030, 14363,   233,  1416,  9036,\n",
              "           2298,  1051,   224, 12934,    65,   640,     7,   100,  1153, 17202,\n",
              "           2538,   693,   233,   704],\n",
              "         [ 1423,  2335,   129,  3484,  1338,  2402,    49,    47,    27,  1346,\n",
              "             82,  3975,   214,  2434,  4659,   473,   159,    79,   684, 14896,\n",
              "           6040,  1826,  1732,    13, 11425,   727,   172,   293,  2286,     5,\n",
              "            421,   126, 14201,   812,  2124,    15,    52,  1119,    17,     3,\n",
              "            896,   303,  4613,     3,    47,   172,  3842,   283,  4803,   258,\n",
              "           1231, 30174,   159,     3,  4617,     7,  1978,   129,  1459,  2352,\n",
              "            153,  5428,   220,     7],\n",
              "         [   45,   858,   293,   151,    18,  1015,    75,  6404,    14,  2081,\n",
              "              9, 29309,   290,     7, 28361,  1346,     3,   363,    14,   136,\n",
              "             15, 16577,   295,  5862,   938,    49,     3,    52,  8691,     3,\n",
              "              3,  2207,  1806,   119,   620,   942,     7,   275,    27,     3,\n",
              "             11,    13,    21,   716,  4668,     7,    82,    15,   575,  5117,\n",
              "          17892,  1183,     3,  2795,     7,   739,     3,   402,  1699,  3764,\n",
              "            664,  3784,    21,   168],\n",
              "         [ 1104,  2248,  2749, 13571,    83,     7,   552,    15,  1281,    15,\n",
              "           2884,   503,    18,   451,    20,   761,     5,    10,  5336,     9,\n",
              "             51,     7,    11,  4463,   167,    15,     5,     2,  3001,     3,\n",
              "             14,  9565,  1611,   552,    15,     9,  8827,  2671,  2900,    17,\n",
              "           2984,  2823,   317,  1088,  3170,  1466,     9,   547,   545,    13,\n",
              "          32992,   427,     5,    15,   665,   306,     3,   172,  1910,   237,\n",
              "           1106,  7436,   970,   421],\n",
              "         [  112,     8,  1541,   342,    42,  2352,  1151,  3922,   364,     3,\n",
              "            565,  1913,  4443,   703,  5553,    15,     3,    59,    13,    59,\n",
              "            469,   441,   357,   159,    46,   322,     3,     1,     2,     3,\n",
              "              3,     8,   545,     2,   273,  1857,    15,    46,     2,  5092,\n",
              "           1255,   910,    14,    35,     8,     9,   262,   972,     2,  1328,\n",
              "             79,     3,     3,   542,  1129,   348,     3,   200,   872,     2,\n",
              "            376,     2,     2,     2],\n",
              "         [    7,  1663,     3,    16,     2,   290,   153,     2,   534,     3,\n",
              "            663,     8,   978,  2572,    15,   692,    63,   681,   303,  9319,\n",
              "              2,     8,   915,     7,  3019,   846,    63,     1,     1,     3,\n",
              "              8,   889,     8,     1,     8,     2,  1201, 50321,     1,    52,\n",
              "           4561,   587, 15679,     4,  7552,   213,  2549,    21,     1,    37,\n",
              "           4557,     3,     3,  3833,  3861,     7,    80,     2,     2,     1,\n",
              "             22,     1,     1,     1],\n",
              "         [13016,     2,     6,     7,     1,    18,  7801,     1,    13,     3,\n",
              "              2,   386,     2,     2,   933,   350,   176,    92,    15,   329,\n",
              "              1,   916,     2,   588, 10497,     2,   176,     1,     1,     3,\n",
              "            109,    20,   824,     1,  4400,     1,   811,   735,     1,  1691,\n",
              "            358,     2,     2,  3480,   679,  1564,   459, 12890,     1,   288,\n",
              "           4746,   294,    63,   316,     7,    62,   125,     1,     1,     1,\n",
              "             24,     1,     1,     1],\n",
              "         [ 2467,     1,     3,    54,     1, 22399,   173,     1, 19936,     3,\n",
              "              1,    13,     1,     1,    17,     2,     2,   281,   476,    56,\n",
              "              1,   249,     1,     2,     2,     1,    11,     1,     1,     3,\n",
              "              3,  1077,   363,     1,  4591,     1,     2,     2,     1,  1303,\n",
              "              2,     1,     1,  3128,     2,     8,     2,  1377,     1,   576,\n",
              "              2,   570,   176,  1834,   424,   773,    11,     1,     1,     1,\n",
              "             23,     1,     1,     1],\n",
              "         [    2,     1,     3,  5513,     1,  4801,  2797,     1,    17,  3957,\n",
              "              1, 19571,     1,     1,   150,     1,     1,     2,    65,   842,\n",
              "              1,     2,     1,     1,     1,     1,  1361,     1,     1,     2,\n",
              "              2,  4338,     2,     1,     2,     1,     1,     1,     1,     2,\n",
              "              1,     1,     1,  2511,     1,  1023,     1,     7,     1,  2941,\n",
              "              1,   673,     2,     2,    49,     2,   211,     1,     1,     1,\n",
              "             75,     1,     1,     1],\n",
              "         [    1,     1,     3,     2,     1,   695,     2,     1,     3,  5049,\n",
              "              1,  2126,     1,     1,   697,     1,     1,     1,   328,  1350,\n",
              "              1,     1,     1,     1,     1,     1,    14,     1,     1,     1,\n",
              "              1,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,   441,     1,   304,     1,   923,     1,     2,\n",
              "              1,     8,     1,     1,   946,     1,   131,     1,     1,     1,\n",
              "              2,     1,     1,     1],\n",
              "         [    1,     1,   197,     1,     1,     2,     1,     1,     3,     2,\n",
              "              1,     2,     1,     1,  6157,     1,     1,     1,     2,     2,\n",
              "              1,     1,     1,     1,     1,     1,  2544,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,  1363,     1,     2,     1,     2,     1,     1,\n",
              "              1,   485,     1,     1,     2,     1,     8,     1,     1,     1,\n",
              "              1,     1,     1,     1],\n",
              "         [    1,     1,     2,     1,     1,     1,     1,     1,     2,     1,\n",
              "              1,     1,     1,     1,     7,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,    94,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     2,     1,     1,     1,     1,     1,     1,\n",
              "              1,     2,     1,     1,     1,     1,   167,     1,     1,     1,\n",
              "              1,     1,     1,     1],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,   556,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,   608,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     2,     1,     1,     1,\n",
              "              1,     1,     1,     1],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     2,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     2,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "t739SN93y7M3",
        "colab_type": "text"
      },
      "source": [
        "### 1.4. Create ModelData "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "XKkup8Kiy7M3",
        "colab_type": "text"
      },
      "source": [
        "In order to use our batched dataset with the FastAI library, we create a `ModelData` object. \n",
        "\n",
        "`ModelData` simply sticks the training, validation (and test) dataset into a single object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTAOcG3AKFX-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c444781d-8508-4421-8f77-0e2d5d225596"
      },
      "source": [
        "ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " abstractive-text-summ.ipynb   environment.yml                README.md\n",
            " \u001b[0m\u001b[01;34mdata\u001b[0m/                         \u001b[01;34mimgs\u001b[0m/                          requirements.txt\n",
            " data-preparation.ipynb        launch-tensorboard_viz.ipynb\n",
            " environment-cpu.yml          \u001b[01;34m'py scripts'\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "3OKhpAcCy7M5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import text module from fastai \n",
        "from fastai.text import *\n",
        "# from fastai import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPi2EsORVA5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelData():\n",
        "    \"\"\"Encapsulates DataLoaders and Datasets for training, validation, test. Base class for fastai *Data classes.\"\"\"\n",
        "    def __init__(self, path, trn_dl, val_dl, test_dl=None):\n",
        "        self.path,self.trn_dl,self.val_dl,self.test_dl = path,trn_dl,val_dl,test_dl\n",
        "\n",
        "    @classmethod\n",
        "    def from_dls(cls, path,trn_dl,val_dl,test_dl=None):\n",
        "        #trn_dl,val_dl = DataLoader(trn_dl),DataLoader(val_dl)\n",
        "        #if test_dl: test_dl = DataLoader(test_dl)\n",
        "        return cls(path, trn_dl, val_dl, test_dl)\n",
        "\n",
        "    @property\n",
        "    def is_reg(self): return self.trn_ds.is_reg\n",
        "    @property\n",
        "    def is_multi(self): return self.trn_ds.is_multi\n",
        "    @property\n",
        "    def trn_ds(self): return self.trn_dl.dataset\n",
        "    @property\n",
        "    def val_ds(self): return self.val_dl.dataset\n",
        "    @property\n",
        "    def test_ds(self): return self.test_dl.dataset\n",
        "    @property\n",
        "    def trn_y(self): return self.trn_ds.y\n",
        "    @property\n",
        "    def val_y(self): return self.val_ds.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "ypfL5bWry7M_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_data = ModelData(SAMPLE_DATA_PATH, trn_dl=train_iter_tuple, val_dl=val_iter_tuple)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Lou0dy4Gy7NF",
        "colab_type": "text"
      },
      "source": [
        "### 1.5. Processed dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Gf3gG8vLy7NG",
        "colab_type": "text"
      },
      "source": [
        "Finally, we are done with processing the dataset: pre-processing, numericalizing, batching and padding.\n",
        "\n",
        "Lets take a look at the final processed data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "RyvEVxWsy7NH",
        "colab_type": "code",
        "outputId": "6adff4a1-812d-4133-fd68-b78668aa8432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#number of batches in training & validation set and number of tokens in vocabulary\n",
        "len(model_data.trn_dl), len(model_data.val_dl), len(TEXT.vocab)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1250, 197, 52221)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "k0Y4vVmcy7NL",
        "colab_type": "code",
        "outputId": "69791614-cfa3-4ed3-e0e7-6af2aacb21f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#shape of one batch in training set (sequence_length x batch_size)\n",
        "t, z = next(model_data.trn_dl.__iter__())\n",
        "t.size(), z.size()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([59, 64]), torch.Size([22, 64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "acrmtdk8y7NO",
        "colab_type": "code",
        "outputId": "b06af79e-99b4-42e4-8d35-9d6e668f8b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "#lets look at an example pair\n",
        "sample_source = t.transpose(1,0)[0].data.cpu().numpy()\n",
        "sample_target = z.transpose(1,0)[0].data.cpu().numpy()\n",
        "\n",
        "print(\"source:\\n%s \\n\\ncorresponding tensor:\\n%s \\n\" %(' '.join([TEXT.vocab.itos[o] for o in sample_source]), sample_source))\n",
        "print(\"target:\\n%s \\n\\ncorresponding tensor:\\n%s \\n\" %(' '.join([TEXT.vocab.itos[o] for o in sample_target]), sample_target))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\n",
            "portraying sen . john f. kerry as wavering and weak , republicans opened their national convention monday by hailing president bush as a leader who had made america safer and , given four more years , could extend that protection to a world shadowed by terrorism . _eos_ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "corresponding tensor:\n",
            "[11438  1313     5   374 ...     1     1     1     1] \n",
            "\n",
            "target:\n",
            "gop opens convention hails bush _eos_ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "corresponding tensor:\n",
            "[2163  754 1970 2779  137    2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fwJoMJhy7NS",
        "colab_type": "text"
      },
      "source": [
        "## 2. Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK9tNFB9y7NT",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Define model architecture\n",
        "The sequence model, consists of:\n",
        "* single layer encoder-decoder RNNs (biGRU-to-uniGRU)\n",
        "* feed-forward attention network (bahdanau)\n",
        "* option for luong global dot attention\n",
        "* option for teacher forcing\n",
        "* option for tied embeddings\n",
        "* option for multi-layer model\n",
        "* option for regularization (dropout)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPfVX6NEy7NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYx31yaBy7NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, rnn_type, input_size, embz_size, hidden_size, batch_size,output_size,max_tgt_len,\n",
        "                 attention_type, tied_weight_type, pre_trained_vector, pre_trained_vector_type, padding_idx,\n",
        "                 num_layers=1, encoder_drop=(0.0,0.0), decoder_drop=(0.0,0.0), \n",
        "                 bidirectional=True, bias=False, teacher_forcing=True):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        rnn_type, attention_type, tied_weight_type = rnn_type.upper(), attention_type.title(), tied_weight_type.lower()\n",
        "        \n",
        "        if rnn_type in ['LSTM', 'GRU']: self.rnn_type = rnn_type\n",
        "        else: raise ValueError(\"\"\"An invalid option for '--rnn_type' was supplied,\n",
        "                                    options are ['LSTM', 'GRU']\"\"\")\n",
        "            \n",
        "        if attention_type in ['Luong', 'Bahdanau']: self.attention_type = attention_type\n",
        "        else: raise ValueError(\"\"\"An invalid option for '--attention_type' was supplied,\n",
        "                                    options are ['Luong', 'Bahdanau']\"\"\")\n",
        "            \n",
        "        if tied_weight_type in ['three_way', 'two_way']: self.tied_weight_type = tied_weight_type\n",
        "        else: raise ValueError(\"\"\"An invalid option for '--tied_weight_type' was supplied,\n",
        "                                    options are ['three_way', 'two_way']\"\"\")\n",
        "    \n",
        "                    \n",
        "        #initialize model parameters            \n",
        "        self.output_size, self.embz_size, self.hidden_size = output_size, embz_size, hidden_size//2\n",
        "        self.num_layers, self.input_size, self.max_tgt_len, self.pre_trained_vector = num_layers, input_size, max_tgt_len, pre_trained_vector\n",
        "        self.bidirectional,self.teacher_forcing, self.pre_trained_vector_type = bidirectional, teacher_forcing, pre_trained_vector_type\n",
        "        self.encoder_drop, self.decoder_drop, self.padding_idx = encoder_drop, decoder_drop, padding_idx\n",
        "        \n",
        "        \n",
        "        if self.teacher_forcing: self.force_prob = 1.0\n",
        "        \n",
        "        #set bidirectional\n",
        "        if self.bidirectional: self.num_directions = 2\n",
        "        else: self.num_directions = 1\n",
        "            \n",
        "        \n",
        "        #encoder\n",
        "        self.encoder_dropout = nn.Dropout(self.encoder_drop[0])\n",
        "        self.encoder_embedding_layer = nn.Embedding(self.input_size, self.embz_size, padding_idx=self.padding_idx)\n",
        "        if self.pre_trained_vector: self.encoder_embedding_layer.weight.data.copy_(self.pre_trained_vector.weight.data)\n",
        "            \n",
        "        self.encoder_rnn = getattr(nn, self.rnn_type)(\n",
        "                           input_size=self.embz_size,\n",
        "                           hidden_size=self.hidden_size,\n",
        "                           num_layers=self.num_layers,\n",
        "                           dropout=self.encoder_drop[1], \n",
        "                           bidirectional=self.bidirectional)\n",
        "        self.encoder_vector_layer = nn.Linear(self.hidden_size*self.num_directions,self.embz_size, bias=bias)\n",
        "        \n",
        "       #decoder\n",
        "        self.decoder_dropout = nn.Dropout(self.decoder_drop[0])\n",
        "        self.decoder_embedding_layer = nn.Embedding(self.input_size, self.embz_size, padding_idx=self.padding_idx)\n",
        "        self.decoder_rnn = getattr(nn, self.rnn_type)(\n",
        "                           input_size=self.embz_size,\n",
        "                           hidden_size=self.hidden_size*self.num_directions,\n",
        "                           num_layers=self.num_layers,\n",
        "                           dropout=self.decoder_drop[1]) \n",
        "        self.decoder_output_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
        "        self.output_layer = nn.Linear(self.embz_size, self.output_size, bias=bias)\n",
        "        \n",
        "        #set tied weights: three way tied weights vs two way tied weights\n",
        "        if self.tied_weight_type == 'three_way':\n",
        "            self.decoder_embedding_layer.weight  = self.encoder_embedding_layer.weight\n",
        "            self.output_layer.weight = self.decoder_embedding_layer.weight  \n",
        "        else:\n",
        "            if self.pre_trained_vector: self.decoder_embedding_layer.weight.data.copy_(self.pre_trained_vector.weight.data)\n",
        "            self.output_layer.weight = self.decoder_embedding_layer.weight  \n",
        "            \n",
        "        #set attention\n",
        "        self.encoder_output_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
        "        self.att_vector_layer = nn.Linear(self.embz_size+self.embz_size, self.embz_size,bias=bias)\n",
        "        if self.attention_type == 'Bahdanau':\n",
        "            self.decoder_hidden_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
        "            self.att_score = nn.Linear(self.embz_size,1,bias=bias)\n",
        "\n",
        "            \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (V(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size)),\n",
        "                    V(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size)))\n",
        "        else:\n",
        "            return V(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size))\n",
        "   \n",
        "\n",
        "    def _cat_directions(self, hidden):\n",
        "        def _cat(h):\n",
        "            return torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n",
        "            \n",
        "        if isinstance(hidden, tuple):\n",
        "            # LSTM hidden contains a tuple (hidden state, cell state)\n",
        "            hidden = tuple([_cat(h) for h in hidden])\n",
        "        else:\n",
        "            # GRU hidden\n",
        "            hidden = _cat(hidden)\n",
        "        return hidden    \n",
        "    \n",
        "    \n",
        "    def bahdanau_attention(self, encoder_output, decoder_hidden, decoder_input):\n",
        "        encoder_output = self.encoder_output_layer(encoder_output) \n",
        "        encoder_output = encoder_output.transpose(0,1)\n",
        "        decoder_hidden = decoder_hidden.transpose(0,1)\n",
        "        att_score = F.tanh(encoder_output + decoder_hidden)\n",
        "        att_score = self.att_score(att_score)\n",
        "        att_weight = F.softmax(att_score, dim=1)\n",
        "        context_vector = torch.bmm(att_weight.transpose(-1, 1), encoder_output).squeeze(1)\n",
        "        att_vector = torch.cat((context_vector, decoder_input), dim=1)\n",
        "        att_vector = self.att_vector_layer(att_vector)\n",
        "        return att_weight.squeeze(-1), att_vector\n",
        "    \n",
        "    \n",
        "    def luong_attention(self, encoder_output, decoder_output):\n",
        "        encoder_output = self.encoder_output_layer(encoder_output) \n",
        "        encoder_output = encoder_output.transpose(0,1)\n",
        "        decoder_output = decoder_output.transpose(0,1)\n",
        "        att_score = torch.bmm(encoder_output, decoder_output.transpose(-1,1))\n",
        "        att_weight = F.softmax(att_score, dim=1)\n",
        "        context_vector = torch.bmm(att_weight.transpose(-1, 1), encoder_output).squeeze(1)\n",
        "        att_vector = torch.cat((context_vector, decoder_output.squeeze(1)), dim=1)\n",
        "        att_vector = self.att_vector_layer(att_vector)\n",
        "        att_vector = F.tanh(att_vector)\n",
        "        return att_weight.squeeze(-1), att_vector\n",
        "        \n",
        "    def decoder_forward(self, batch_size, encoder_output, decoder_hidden, y=None):\n",
        "        decoder_input = V(torch.zeros(batch_size).long())  \n",
        "        output_seq_stack, att_stack = [], []\n",
        "        \n",
        "        for i in range(self.max_tgt_len):\n",
        "            decoder_input = self.decoder_dropout(self.decoder_embedding_layer(decoder_input))\n",
        "            if self.attention_type == 'Bahdanau':\n",
        "                if isinstance(decoder_hidden, tuple):\n",
        "                    prev_hidden = self.decoder_hidden_layer(decoder_hidden[0][-1]).unsqueeze(0)\n",
        "                else:\n",
        "                    prev_hidden = self.decoder_hidden_layer(decoder_hidden[-1]).unsqueeze(0) \n",
        "                att, decoder_input = self.bahdanau_attention(encoder_output, prev_hidden, decoder_input)\n",
        "                decoder_output, decoder_hidden = self.decoder_rnn(decoder_input.unsqueeze(0), decoder_hidden)\n",
        "                decoder_output = self.decoder_output_layer(decoder_output.squeeze(0)) \n",
        "            else:\n",
        "                decoder_output, decoder_hidden = self.decoder_rnn(decoder_input.unsqueeze(0), decoder_hidden)\n",
        "                decoder_output = self.decoder_output_layer(decoder_output) \n",
        "                att, decoder_output = self.luong_attention(encoder_output, decoder_output)\n",
        "            att_stack.append(att)\n",
        "            output = self.output_layer(decoder_output)\n",
        "            output_seq_stack.append(output)\n",
        "            decoder_input = V(output.data.max(1)[1])\n",
        "            if (decoder_input==1).all(): break \n",
        "            if self.teacher_forcing:    \n",
        "                samp_prob = round(random.random(),1)\n",
        "                if (y is not None) and (samp_prob < self.force_prob):\n",
        "                    if i >= len(y): break\n",
        "                    decoder_input = y[i] \n",
        "                \n",
        "        return torch.stack(output_seq_stack), torch.stack(att_stack)\n",
        "        \n",
        "                \n",
        "    def forward(self, seq, y=None):\n",
        "        batch_size = seq[0].size(0)\n",
        "        encoder_hidden = self.init_hidden(batch_size)\n",
        "        encoder_input = self.encoder_dropout(self.encoder_embedding_layer(seq))\n",
        "        encoder_output, encoder_hidden = self.encoder_rnn(encoder_input, encoder_hidden) \n",
        "        if self.bidirectional:\n",
        "            encoder_hidden = self._cat_directions(encoder_hidden)\n",
        "        output = self.decoder_forward(batch_size, encoder_output, encoder_hidden, y=y)\n",
        "        if isinstance(encoder_hidden, tuple):\n",
        "            encoder_vector = self.encoder_vector_layer(encoder_hidden[0][-1])\n",
        "        else:\n",
        "            encoder_vector = self.encoder_vector_layer(encoder_hidden[-1])\n",
        "        output = output + (encoder_vector,)  \n",
        "        return output\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMSJWXyVy7Nb",
        "colab_type": "code",
        "outputId": "510287f6-1acf-4538-cbca-e04c0c663c2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#set maximum target summary size \n",
        "its = [next(model_data.trn_dl.__iter__())[1] for i in range(10)]\n",
        "max_tgt_len = int(np.percentile([its[o].size()[0] for o in range(len(its))], 99))\n",
        "max_tgt_len"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd7UmiVhy7Ni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_pickle(path, filename, file):\n",
        "    \"\"\"Function to save file as pickle\"\"\"\n",
        "    with open(f'{path}/{filename}', 'wb') as f:\n",
        "        pickle.dump(file, f)\n",
        "\n",
        "        \n",
        "def norm_pre_trained_embeddings(vecs, itos, em_sz, padding_idx):\n",
        "    \"\"\"Function to load and normalize pretrained vectors\"\"\"\n",
        "    emb = nn.Embedding(len(itos), em_sz, padding_idx=padding_idx)\n",
        "    wgts = emb.weight.data\n",
        "    for i,w in enumerate(itos):\n",
        "        try: \n",
        "            wgts[i] = torch.from_numpy(vecs[w]-vec_mean)\n",
        "            wgts[i] = torch.from_numpy(vecs[w]/vec_std)\n",
        "        except: pass \n",
        "    emb.weight.requires_grad = False    \n",
        "    return emb\n",
        "\n",
        "\n",
        "def embedding_param(path, data_field, pre_trained_vector_type, embz_size=128, save_vocab=False, itos='itos', stoi='stoi'):\n",
        "    \"\"\"Returns embedding parameters\"\"\"\n",
        "    pre_trained=None\n",
        "    padding_idx = data_field.vocab.stoi['<pad>']\n",
        "    index_to_string, string_to_index = data_field.vocab.itos, data_field.vocab.stoi\n",
        "    if save_vocab:\n",
        "        vocab_path = os.path.join(path, \"vocab\")\n",
        "        os.makedirs(vocab_path, exist_ok=True)\n",
        "        save_pickle(vocab_path, f'{itos}.pk', index_to_string) \n",
        "        save_pickle(vocab_path, f'{stoi}.pk', string_to_index) \n",
        "    if pre_trained_vector_type:\n",
        "        vec_mean, vec_std = data_field.vocab.vectors.numpy().mean(), data_field.vocab.vectors.numpy().std()\n",
        "        print('pre_trained_vector_mean = %s, pre_trained_vector_std = %s'%(vec_mean, vec_std))\n",
        "        vector_weight_matrix = data_field.vocab.vectors\n",
        "        embz_size = vector_weight_matrix.size(1)\n",
        "        pre_trained = norm_pre_trained_embeddings(vector_weight_matrix, index_to_string, embz_size, padding_idx)\n",
        "        print('Normalizing.... \\npre_trained_vector_mean = %s, pre_trained_vector_std = %s' %(pre_trained.weight.data.numpy().mean(), pre_trained.weight.data.numpy().std()))\n",
        "    return pre_trained, embz_size, padding_idx\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX7zbUrYy7Nm",
        "colab_type": "code",
        "outputId": "bb8375f9-091f-49ea-c414-7c2b4091cd7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rev=0\n",
        "rev += 1\n",
        "print(\"rev = %s\" %rev)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rev = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzb2LLYXy7Nt",
        "colab_type": "code",
        "outputId": "4f093151-5c3e-4d63-eec3-6968b04300f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "pre_trained_vector,  embz_size, padding_idx = embedding_param(SAMPLE_DATA_PATH, TEXT, pre_trained_vector_type, save_vocab=True)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pre_trained_vector_mean = 0.0019917963, pre_trained_vector_std = 0.43600857\n",
            "Normalizing.... \n",
            "pre_trained_vector_mean = 0.000612008, pre_trained_vector_std = 1.0000719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWP2Vlzby7Nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = len(TEXT.vocab)\n",
        "hidden_size = 400\n",
        "output_size =  len(TEXT.vocab)\n",
        "rnn_type = 'gru'\n",
        "tied_weight_type ='three_way'\n",
        "max_tgt_len = max_tgt_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbycPRhWy7N2",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw4_P1zQy7N4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Stepper():\n",
        "    def __init__(self, m, opt, crit, clip=0, reg_fn=None, fp16=False, loss_scale=1):\n",
        "        self.m,self.opt,self.crit,self.clip,self.reg_fn = m,opt,crit,clip,reg_fn\n",
        "        self.fp16 = fp16\n",
        "        self.reset(True)\n",
        "        if self.fp16: self.fp32_params = copy_model_to_fp32(m, opt)\n",
        "        self.loss_scale = loss_scale\n",
        "\n",
        "    def reset(self, train=True):\n",
        "        if train: apply_leaf(self.m, set_train_mode)\n",
        "        else: self.m.eval()\n",
        "        if hasattr(self.m, 'reset'):\n",
        "            self.m.reset()\n",
        "            if self.fp16: self.fp32_params = copy_model_to_fp32(self.m, self.opt)\n",
        "\n",
        "    def step(self, xs, y, epoch):\n",
        "        xtra = []\n",
        "        output = self.m(*xs)\n",
        "        if isinstance(output,tuple): output,*xtra = output\n",
        "        if self.fp16: self.m.zero_grad()\n",
        "        else: self.opt.zero_grad() \n",
        "        loss = raw_loss = self.crit(output, y)\n",
        "        if self.loss_scale != 1: assert(self.fp16); loss = loss*self.loss_scale\n",
        "        if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n",
        "        loss.backward()\n",
        "        if self.fp16: update_fp32_grads(self.fp32_params, self.m)\n",
        "        if self.loss_scale != 1:\n",
        "            for param in self.fp32_params: param.grad.data.div_(self.loss_scale)\n",
        "        if self.clip:   # Gradient clipping\n",
        "            nn.utils.clip_grad_norm(trainable_params_(self.m), self.clip)\n",
        "        self.opt.step()\n",
        "        if self.fp16: \n",
        "            copy_fp32_toStepper_model(self.m, self.fp32_params)\n",
        "            torch.cuda.synchronize()\n",
        "        return torch_item(raw_loss.data)\n",
        "\n",
        "    def evaluate(self, xs, y):\n",
        "        preds = self.m(*xs)\n",
        "        if isinstance(preds,tuple): preds=preds[0]\n",
        "        return preds, self.crit(preds, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ETFYS_9Teuu2",
        "colab": {}
      },
      "source": [
        "class Callback:\n",
        "    '''\n",
        "    An abstract class that all callback(e.g., LossRecorder) classes extends from. \n",
        "    Must be extended before usage.\n",
        "    '''\n",
        "    def on_train_begin(self): pass\n",
        "    def on_batch_begin(self): pass\n",
        "    def on_phase_begin(self): pass\n",
        "    def on_epoch_end(self, metrics): pass\n",
        "    def on_phase_end(self): pass\n",
        "    def on_batch_end(self, metrics): pass\n",
        "    def on_train_end(self): pass\n",
        "\n",
        "    \n",
        "class DecayScheduler():\n",
        "    '''Given initial and endvalue, this class generates the next value depending on decay type and number of iterations. (by calling next_val().) '''\n",
        "\n",
        "    def __init__(self, dec_type, num_it, start_val, end_val=None, extra=None):\n",
        "        self.dec_type, self.nb, self.start_val, self.end_val, self.extra = dec_type, num_it, start_val, end_val, extra\n",
        "        self.it = 0\n",
        "        if self.end_val is None and not (self.dec_type in [1,4]): self.end_val = 0\n",
        "    \n",
        "    def next_val(self):\n",
        "        self.it += 1\n",
        "        if self.dec_type == DecayType.NO:\n",
        "            return self.start_val\n",
        "        elif self.dec_type == DecayType.LINEAR:\n",
        "            pct = self.it/self.nb\n",
        "            return self.start_val + pct * (self.end_val-self.start_val)\n",
        "        elif self.dec_type == DecayType.COSINE:\n",
        "            cos_out = np.cos(np.pi*(self.it)/self.nb) + 1\n",
        "            return self.end_val + (self.start_val-self.end_val) / 2 * cos_out\n",
        "        elif self.dec_type == DecayType.EXPONENTIAL:\n",
        "            ratio = self.end_val / self.start_val\n",
        "            return self.start_val * (ratio **  (self.it/self.nb))\n",
        "        elif self.dec_type == DecayType.POLYNOMIAL:\n",
        "            return self.end_val + (self.start_val-self.end_val) * (1 - self.it/self.nb)**self.extra"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBUlaEvzy7N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Learner():\n",
        "    def __init__(self, data, models, opt_fn=None, tmp_name='tmp', models_name='models', metrics=None, clip=None, crit=None):\n",
        "        \"\"\"\n",
        "        Combines a ModelData object with a nn.Module object, such that you can train that\n",
        "        module.\n",
        "        data (ModelData): An instance of ModelData.\n",
        "        models(module): chosen neural architecture for solving a supported problem.\n",
        "        opt_fn(function): optimizer function, uses SGD with Momentum of .9 if none.\n",
        "        tmp_name(str): output name of the directory containing temporary files from training process\n",
        "        models_name(str): output name of the directory containing the trained model\n",
        "        metrics(list): array of functions for evaluating a desired metric. Eg. accuracy.\n",
        "        clip(float): gradient clip chosen to limit the change in the gradient to prevent exploding gradients Eg. .3\n",
        "        \"\"\"\n",
        "        self.data_,self.models,self.metrics,self.clip = data,models,metrics,clip\n",
        "        self.sched=None\n",
        "        self.wd_sched = None\n",
        "        self.opt_fn = opt_fn or SGD_Momentum(0.9)\n",
        "        self.tmp_path = tmp_name if os.path.isabs(tmp_name) else os.path.join(self.data.path, tmp_name)\n",
        "        self.models_path = models_name if os.path.isabs(models_name) else os.path.join(self.data.path, models_name)\n",
        "        os.makedirs(self.tmp_path, exist_ok=True)\n",
        "        os.makedirs(self.models_path, exist_ok=True)\n",
        "        self.crit = crit if crit else self._get_crit(data)\n",
        "        self.reg_fn = None\n",
        "        self.fp16 = False\n",
        "\n",
        "    @classmethod\n",
        "    def from_model_data(cls, m, data, **kwargs):\n",
        "        self = cls(data, BasicModel(to_gpu(m)), **kwargs)\n",
        "        self.unfreeze()\n",
        "        return self\n",
        "\n",
        "    def __getitem__(self,i): return self.children[i]\n",
        "\n",
        "    @property\n",
        "    def children(self): return children(self.model)\n",
        "\n",
        "    @property\n",
        "    def model(self): return self.models.model\n",
        "\n",
        "    @property\n",
        "    def data(self): return self.data_\n",
        "\n",
        "    def summary(self): return model_summary(self.model, [torch.rand(3, 3, self.data.sz,self.data.sz)])\n",
        "\n",
        "    def __repr__(self): return self.model.__repr__()\n",
        "    \n",
        "    def lsuv_init(self, needed_std=1.0, std_tol=0.1, max_attempts=10, do_orthonorm=False):         \n",
        "        x = V(next(iter(self.data.trn_dl))[0])\n",
        "        self.models.model=apply_lsuv_init(self.model, x, needed_std=needed_std, std_tol=std_tol,\n",
        "                            max_attempts=max_attempts, do_orthonorm=do_orthonorm, \n",
        "                            cuda=USE_GPU and torch.cuda.is_available())\n",
        "\n",
        "    def set_bn_freeze(self, m, do_freeze):\n",
        "        if hasattr(m, 'running_mean'): m.bn_freeze = do_freeze\n",
        "\n",
        "    def bn_freeze(self, do_freeze):\n",
        "        apply_leaf(self.model, lambda m: self.set_bn_freeze(m, do_freeze))\n",
        "\n",
        "    def freeze_to(self, n):\n",
        "        c=self.get_layer_groups()\n",
        "        for l in c:     set_trainable(l, False)\n",
        "        for l in c[n:]: set_trainable(l, True)\n",
        "\n",
        "    def freeze_all_but(self, n):\n",
        "        c=self.get_layer_groups()\n",
        "        for l in c: set_trainable(l, False)\n",
        "        set_trainable(c[n], True)\n",
        "        \n",
        "    def freeze_groups(self, groups):\n",
        "        c = self.get_layer_groups()\n",
        "        self.unfreeze()\n",
        "        for g in groups:\n",
        "            set_trainable(c[g], False)\n",
        "            \n",
        "    def unfreeze_groups(self, groups):\n",
        "        c = self.get_layer_groups()\n",
        "        for g in groups:\n",
        "            set_trainable(c[g], True)\n",
        "\n",
        "    def unfreeze(self): self.freeze_to(0)\n",
        "\n",
        "    def get_model_path(self, name): return os.path.join(self.models_path,name)+'.h5'\n",
        "    \n",
        "    def save(self, name): \n",
        "        save_model(self.model, self.get_model_path(name))\n",
        "        if hasattr(self, 'swa_model'): save_model(self.swa_model, self.get_model_path(name)[:-3]+'-swa.h5')\n",
        "                       \n",
        "    def load(self, name): \n",
        "        load_model(self.model, self.get_model_path(name))\n",
        "        if hasattr(self, 'swa_model'): load_model(self.swa_model, self.get_model_path(name)[:-3]+'-swa.h5')\n",
        "\n",
        "    def set_data(self, data): self.data_ = data\n",
        "\n",
        "    def get_cycle_end(self, name):\n",
        "        if name is None: return None\n",
        "        return lambda sched, cycle: self.save_cycle(name, cycle)\n",
        "\n",
        "    def save_cycle(self, name, cycle): self.save(f'{name}_cyc_{cycle}')\n",
        "    def load_cycle(self, name, cycle): self.load(f'{name}_cyc_{cycle}')\n",
        "\n",
        "    def half(self):\n",
        "        if self.fp16: return\n",
        "        self.fp16 = True\n",
        "        if type(self.model) != FP16: self.models.model = FP16(self.model)\n",
        "    def float(self):\n",
        "        if not self.fp16: return\n",
        "        self.fp16 = False\n",
        "        if type(self.model) == FP16: self.models.model = self.model.module\n",
        "        self.model.float()\n",
        "\n",
        "    def fit_gen(self, model, data, layer_opt, n_cycle, cycle_len=None, cycle_mult=1, cycle_save_name=None, best_save_name=None,\n",
        "                use_clr=None, use_clr_beta=None, metrics=None, callbacks=None, use_wd_sched=False, norm_wds=False,             \n",
        "                wds_sched_mult=None, use_swa=False, swa_start=1, swa_eval_freq=5, **kwargs):\n",
        "\n",
        "        \"\"\"Method does some preparation before finally delegating to the 'fit' method for\n",
        "        fitting the model. Namely, if cycle_len is defined, it adds a 'Cosine Annealing'\n",
        "        scheduler for varying the learning rate across iterations.\n",
        "        Method also computes the total number of epochs to fit based on provided 'cycle_len',\n",
        "        'cycle_mult', and 'n_cycle' parameters.\n",
        "        Args:\n",
        "            model (Learner):  Any neural architecture for solving a supported problem.\n",
        "                Eg. ResNet-34, RNN_Learner etc.\n",
        "            data (ModelData): An instance of ModelData.\n",
        "            layer_opt (LayerOptimizer): An instance of the LayerOptimizer class\n",
        "            n_cycle (int): number of cycles\n",
        "            cycle_len (int):  number of epochs before lr is reset to the initial value.\n",
        "                E.g if cycle_len = 3, then the lr is varied between a maximum\n",
        "                and minimum value over 3 epochs.\n",
        "            cycle_mult (int): additional parameter for influencing how the lr resets over\n",
        "                the cycles. For an intuitive explanation, please see\n",
        "                https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1.ipynb\n",
        "            cycle_save_name (str): use to save the weights at end of each cycle (requires\n",
        "                use_clr, use_clr_beta or cycle_len arg)\n",
        "            best_save_name (str): use to save weights of best model during training.\n",
        "            metrics (function): some function for evaluating a desired metric. Eg. accuracy.\n",
        "            callbacks (list(Callback)): callbacks to apply during the training.\n",
        "            use_wd_sched (bool, optional): set to True to enable weight regularization using\n",
        "                the technique mentioned in https://arxiv.org/abs/1711.05101. When this is True\n",
        "                alone (see below), the regularization is detached from gradient update and\n",
        "                applied directly to the weights.\n",
        "            norm_wds (bool, optional): when this is set to True along with use_wd_sched, the\n",
        "                regularization factor is normalized with each training cycle.\n",
        "            wds_sched_mult (function, optional): when this is provided along with use_wd_sched\n",
        "                as True, the value computed by this function is multiplied with the regularization\n",
        "                strength. This function is passed the WeightDecaySchedule object. And example\n",
        "                function that can be passed is:\n",
        "                            f = lambda x: np.array(x.layer_opt.lrs) / x.init_lrs\n",
        "                            \n",
        "            use_swa (bool, optional): when this is set to True, it will enable the use of\n",
        "                Stochastic Weight Averaging (https://arxiv.org/abs/1803.05407). The learner will\n",
        "                include an additional model (in the swa_model attribute) for keeping track of the \n",
        "                average weights as described in the paper. All testing of this technique so far has\n",
        "                been in image classification, so use in other contexts is not guaranteed to work.\n",
        "                \n",
        "            swa_start (int, optional): if use_swa is set to True, then this determines the epoch\n",
        "                to start keeping track of the average weights. It is 1-indexed per the paper's\n",
        "                conventions.\n",
        "                \n",
        "            swa_eval_freq (int, optional): if use_swa is set to True, this determines the frequency\n",
        "                at which to evaluate the performance of the swa_model. This evaluation can be costly\n",
        "                for models using BatchNorm (requiring a full pass through the data), which is why the\n",
        "                default is not to evaluate after each epoch.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "\n",
        "        if cycle_save_name:\n",
        "            assert use_clr or use_clr_beta or cycle_len, \"cycle_save_name argument requires either of the following arguments use_clr, use_clr_beta, cycle_len\"\n",
        "\n",
        "        if callbacks is None: callbacks=[]\n",
        "        if metrics is None: metrics=self.metrics\n",
        "\n",
        "        if use_wd_sched:\n",
        "            # This needs to come before CosAnneal() because we need to read the initial learning rate from\n",
        "            # layer_opt.lrs - but CosAnneal() alters the layer_opt.lrs value initially (divides by 100)\n",
        "            if np.sum(layer_opt.wds) == 0:\n",
        "                print('fit() warning: use_wd_sched is set to True, but weight decay(s) passed are 0. Use wds to '\n",
        "                      'pass weight decay values.')\n",
        "            batch_per_epoch = len(data.trn_dl)\n",
        "            cl = cycle_len if cycle_len else 1\n",
        "            self.wd_sched = WeightDecaySchedule(layer_opt, batch_per_epoch, cl, cycle_mult, n_cycle,\n",
        "                                                norm_wds, wds_sched_mult)\n",
        "            callbacks += [self.wd_sched]\n",
        "\n",
        "        if use_clr is not None:\n",
        "            clr_div,cut_div = use_clr[:2]\n",
        "            moms = use_clr[2:] if len(use_clr) > 2 else None\n",
        "            cycle_end = self.get_cycle_end(cycle_save_name)\n",
        "            assert cycle_len, \"use_clr requires cycle_len arg\"\n",
        "            self.sched = CircularLR(layer_opt, len(data.trn_dl)*cycle_len, on_cycle_end=cycle_end, div=clr_div, cut_div=cut_div,\n",
        "                                    momentums=moms)\n",
        "        elif use_clr_beta is not None:\n",
        "            div,pct = use_clr_beta[:2]\n",
        "            moms = use_clr_beta[2:] if len(use_clr_beta) > 3 else None\n",
        "            cycle_end = self.get_cycle_end(cycle_save_name)\n",
        "            assert cycle_len, \"use_clr_beta requires cycle_len arg\"\n",
        "            self.sched = CircularLR_beta(layer_opt, len(data.trn_dl)*cycle_len, on_cycle_end=cycle_end, div=div,\n",
        "                                    pct=pct, momentums=moms)\n",
        "        elif cycle_len:\n",
        "            cycle_end = self.get_cycle_end(cycle_save_name)\n",
        "            cycle_batches = len(data.trn_dl)*cycle_len\n",
        "            self.sched = CosAnneal(layer_opt, cycle_batches, on_cycle_end=cycle_end, cycle_mult=cycle_mult)\n",
        "        elif not self.sched: self.sched=LossRecorder(layer_opt)\n",
        "        callbacks+=[self.sched]\n",
        "\n",
        "        if best_save_name is not None:\n",
        "            callbacks+=[SaveBestModel(self, layer_opt, metrics, best_save_name)]\n",
        "\n",
        "        if use_swa:\n",
        "            # make a copy of the model to track average weights\n",
        "            self.swa_model = copy.deepcopy(model)\n",
        "            callbacks+=[SWA(model, self.swa_model, swa_start)]\n",
        "\n",
        "        n_epoch = int(sum_geom(cycle_len if cycle_len else 1, cycle_mult, n_cycle))\n",
        "        return fit(model, data, n_epoch, layer_opt.opt, self.crit,\n",
        "            metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, fp16=self.fp16,\n",
        "            swa_model=self.swa_model if use_swa else None, swa_start=swa_start, \n",
        "            swa_eval_freq=swa_eval_freq, **kwargs)\n",
        "\n",
        "    def get_layer_groups(self): return self.models.get_layer_groups()\n",
        "\n",
        "    def get_layer_opt(self, lrs, wds):\n",
        "\n",
        "        \"\"\"Method returns an instance of the LayerOptimizer class, which\n",
        "        allows for setting differential learning rates for different\n",
        "        parts of the model.\n",
        "        An example of how a model maybe differentiated into different parts\n",
        "        for application of differential learning rates and weight decays is\n",
        "        seen in ../.../courses/dl1/fastai/conv_learner.py, using the dict\n",
        "        'model_meta'. Currently, this seems supported only for convolutional\n",
        "        networks such as VGG-19, ResNet-XX etc.\n",
        "        Args:\n",
        "            lrs (float or list(float)): learning rate(s) for the model\n",
        "            wds (float or list(float)): weight decay parameter(s).\n",
        "        Returns:\n",
        "            An instance of a LayerOptimizer\n",
        "        \"\"\"\n",
        "        return LayerOptimizer(self.opt_fn, self.get_layer_groups(), lrs, wds)\n",
        "\n",
        "    def fit(self, lrs, n_cycle, wds=None, **kwargs):\n",
        "\n",
        "        \"\"\"Method gets an instance of LayerOptimizer and delegates to self.fit_gen(..)\n",
        "        Note that one can specify a list of learning rates which, when appropriately\n",
        "        defined, will be applied to different segments of an architecture. This seems\n",
        "        mostly relevant to ImageNet-trained models, where we want to alter the layers\n",
        "        closest to the images by much smaller amounts.\n",
        "        Likewise, a single or list of weight decay parameters can be specified, which\n",
        "        if appropriate for a model, will apply variable weight decay parameters to\n",
        "        different segments of the model.\n",
        "        Args:\n",
        "            lrs (float or list(float)): learning rate for the model\n",
        "            n_cycle (int): number of cycles (or iterations) to fit the model for\n",
        "            wds (float or list(float)): weight decay parameter(s).\n",
        "            kwargs: other arguments\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.sched = None\n",
        "        layer_opt = self.get_layer_opt(lrs, wds)\n",
        "        return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)\n",
        "\n",
        "    def warm_up(self, lr, wds=None):\n",
        "        layer_opt = self.get_layer_opt(lr/4, wds)\n",
        "        self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), lr, linear=True)\n",
        "        return self.fit_gen(self.model, self.data, layer_opt, 1)\n",
        "\n",
        "    def lr_find(self, start_lr=1e-5, end_lr=10, wds=None, linear=False, **kwargs):\n",
        "        \"\"\"Helps you find an optimal learning rate for a model.\n",
        "         It uses the technique developed in the 2015 paper\n",
        "         `Cyclical Learning Rates for Training Neural Networks`, where\n",
        "         we simply keep increasing the learning rate from a very small value,\n",
        "         until the loss starts decreasing.\n",
        "        Args:\n",
        "            start_lr (float/numpy array) : Passing in a numpy array allows you\n",
        "                to specify learning rates for a learner's layer_groups\n",
        "            end_lr (float) : The maximum learning rate to try.\n",
        "            wds (iterable/float)\n",
        "        Examples:\n",
        "            As training moves us closer to the optimal weights for a model,\n",
        "            the optimal learning rate will be smaller. We can take advantage of\n",
        "            that knowledge and provide lr_find() with a starting learning rate\n",
        "            1000x smaller than the model's current learning rate as such:\n",
        "            >> learn.lr_find(lr/1000)\n",
        "            >> lrs = np.array([ 1e-4, 1e-3, 1e-2 ])\n",
        "            >> learn.lr_find(lrs / 1000)\n",
        "        Notes:\n",
        "            lr_find() may finish before going through each batch of examples if\n",
        "            the loss decreases enough.\n",
        "        .. _Cyclical Learning Rates for Training Neural Networks:\n",
        "            http://arxiv.org/abs/1506.01186\n",
        "        \"\"\"\n",
        "        self.save('tmp')\n",
        "        layer_opt = self.get_layer_opt(start_lr, wds)\n",
        "        self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), end_lr, linear=linear)\n",
        "        self.fit_gen(self.model, self.data, layer_opt, 1, **kwargs)\n",
        "        self.load('tmp')\n",
        "        \n",
        "\n",
        "    def lr_find2(self, start_lr=1e-5, end_lr=10, num_it = 100, wds=None, linear=False, stop_dv=True, **kwargs):\n",
        "        \"\"\"A variant of lr_find() that helps find the best learning rate. It doesn't do\n",
        "        an epoch but a fixed num of iterations (which may be more or less than an epoch\n",
        "        depending on your data).\n",
        "        At each step, it computes the validation loss and the metrics on the next\n",
        "        batch of the validation data, so it's slower than lr_find().\n",
        "        Args:\n",
        "            start_lr (float/numpy array) : Passing in a numpy array allows you\n",
        "                to specify learning rates for a learner's layer_groups\n",
        "            end_lr (float) : The maximum learning rate to try.\n",
        "            num_it : the number of iterations you want it to run\n",
        "            wds (iterable/float)\n",
        "            stop_dv : stops (or not) when the losses starts to explode.\n",
        "        \"\"\"\n",
        "        self.save('tmp')\n",
        "        layer_opt = self.get_layer_opt(start_lr, wds)\n",
        "        self.sched = LR_Finder2(layer_opt, num_it, end_lr, linear=linear, metrics=self.metrics, stop_dv=stop_dv)\n",
        "        self.fit_gen(self.model, self.data, layer_opt, num_it//len(self.data.trn_dl) + 1, all_val=True, **kwargs)\n",
        "        self.load('tmp')\n",
        "\n",
        "    def predict(self, is_test=False, use_swa=False):\n",
        "        dl = self.data.test_dl if is_test else self.data.val_dl\n",
        "        m = self.swa_model if use_swa else self.model\n",
        "        return predict(m, dl)\n",
        "\n",
        "    def predict_with_targs(self, is_test=False, use_swa=False):\n",
        "        dl = self.data.test_dl if is_test else self.data.val_dl\n",
        "        m = self.swa_model if use_swa else self.model\n",
        "        return predict_with_targs(m, dl)\n",
        "\n",
        "    def predict_dl(self, dl): return predict_with_targs(self.model, dl)[0]\n",
        "\n",
        "    def predict_array(self, arr):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            arr: a numpy array to be used as input to the model for prediction purposes\n",
        "        Returns:\n",
        "            a numpy array containing the predictions from the model\n",
        "        \"\"\"\n",
        "        if not isinstance(arr, np.ndarray): raise OSError(f'Not valid numpy array')\n",
        "        self.model.eval()\n",
        "        return to_np(self.model(to_gpu(V(T(arr)))))\n",
        "\n",
        "    def TTA(self, n_aug=4, is_test=False):\n",
        "        \"\"\" Predict with Test Time Augmentation (TTA)\n",
        "        Additional to the original test/validation images, apply image augmentation to them\n",
        "        (just like for training images) and calculate the mean of predictions. The intent\n",
        "        is to increase the accuracy of predictions by examining the images using multiple\n",
        "        perspectives.\n",
        "            n_aug: a number of augmentation images to use per original image\n",
        "            is_test: indicate to use test images; otherwise use validation images\n",
        "        Returns:\n",
        "            (tuple): a tuple containing:\n",
        "                log predictions (numpy.ndarray): log predictions (i.e. `np.exp(log_preds)` will return probabilities)\n",
        "                targs (numpy.ndarray): target values when `is_test==False`; zeros otherwise.\n",
        "        \"\"\"\n",
        "        dl1 = self.data.test_dl     if is_test else self.data.val_dl\n",
        "        dl2 = self.data.test_aug_dl if is_test else self.data.aug_dl\n",
        "        preds1,targs = predict_with_targs(self.model, dl1)\n",
        "        preds1 = [preds1]*math.ceil(n_aug/4)\n",
        "        preds2 = [predict_with_targs(self.model, dl2)[0] for i in tqdm(range(n_aug), leave=False)]\n",
        "        return np.stack(preds1+preds2), targs\n",
        "\n",
        "    def fit_opt_sched(self, phases, cycle_save_name=None, best_save_name=None, stop_div=False, data_list=None, callbacks=None, \n",
        "                      cut = None, use_swa=False, swa_start=1, swa_eval_freq=5, **kwargs):\n",
        "        \"\"\"Wraps us the content of phases to send them to model.fit(..)\n",
        "        This will split the training in several parts, each with their own learning rates/\n",
        "        wds/momentums/optimizer detailed in phases.\n",
        "        Additionaly we can add a list of different data objets in data_list to train\n",
        "        on different datasets (to change the size for instance) for each of these groups.\n",
        "        Args:\n",
        "            phases: a list of TrainingPhase objects\n",
        "            stop_div: when True, stops the training if the loss goes too high\n",
        "            data_list: a list of different Data objects.\n",
        "            kwargs: other arguments\n",
        "            use_swa (bool, optional): when this is set to True, it will enable the use of\n",
        "                Stochastic Weight Averaging (https://arxiv.org/abs/1803.05407). The learner will\n",
        "                include an additional model (in the swa_model attribute) for keeping track of the \n",
        "                average weights as described in the paper. All testing of this technique so far has\n",
        "                been in image classification, so use in other contexts is not guaranteed to work. \n",
        "            swa_start (int, optional): if use_swa is set to True, then this determines the epoch\n",
        "                to start keeping track of the average weights. It is 1-indexed per the paper's\n",
        "                conventions.\n",
        "            swa_eval_freq (int, optional): if use_swa is set to True, this determines the frequency\n",
        "                at which to evaluate the performance of the swa_model. This evaluation can be costly\n",
        "                for models using BatchNorm (requiring a full pass through the data), which is why the\n",
        "                default is not to evaluate after each epoch.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        if data_list is None: data_list=[]\n",
        "        if callbacks is None: callbacks=[]\n",
        "        layer_opt = LayerOptimizer(phases[0].opt_fn, self.get_layer_groups(), 1e-2, phases[0].wds)\n",
        "        if len(data_list) == 0: nb_batches = [len(self.data.trn_dl)] * len(phases)\n",
        "        else: nb_batches = [len(data.trn_dl) for data in data_list] \n",
        "        self.sched = OptimScheduler(layer_opt, phases, nb_batches, stop_div)\n",
        "        callbacks.append(self.sched)\n",
        "        metrics = self.metrics\n",
        "        if best_save_name is not None:\n",
        "            callbacks+=[SaveBestModel(self, layer_opt, metrics, best_save_name)]\n",
        "        if use_swa:\n",
        "            # make a copy of the model to track average weights\n",
        "            self.swa_model = copy.deepcopy(self.model)\n",
        "            callbacks+=[SWA(self.model, self.swa_model, swa_start)]\n",
        "        n_epochs = [phase.epochs for phase in phases] if cut is None else cut\n",
        "        if len(data_list)==0: data_list = [self.data]\n",
        "        return fit(self.model, data_list, n_epochs,layer_opt, self.crit,\n",
        "            metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, fp16=self.fp16,\n",
        "            swa_model=self.swa_model if use_swa else None, swa_start=swa_start, \n",
        "            swa_eval_freq=swa_eval_freq, **kwargs)\n",
        "\n",
        "    def _get_crit(self, data): return F.mse_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1BRJGoGdz5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(m, p): torch.save(m.state_dict(), p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdFjvuN8y7OB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#custom callbacks\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "class TensorboardLogger(Callback):\n",
        "    def __init__(self, path, log_name, metrics_names=[]):\n",
        "        super().__init__()\n",
        "        self.metrics_names = [\"validation_loss\"]\n",
        "        self.metrics_names += metrics_names\n",
        "        log_path = os.path.join(path, \"logs\")\n",
        "        self.log_dir = os.path.join(log_path, log_name)\n",
        "        if os.path.exists(self.log_dir): shutil.rmtree(self.log_dir)\n",
        "        os.makedirs(self.log_dir)\n",
        "        \n",
        "    def on_train_begin(self):\n",
        "        self.iteration = 0\n",
        "        self.epoch = 0\n",
        "        self.writer = SummaryWriter(log_dir=self.log_dir)\n",
        "    def on_batch_begin(self): pass\n",
        "    def on_phase_begin(self): pass\n",
        "    def on_epoch_end(self, metrics):\n",
        "        self.epoch += 1\n",
        "        for val, name in zip(metrics, self.metrics_names):\n",
        "            self.writer.add_scalar(name, val, self.iteration) \n",
        "                        \n",
        "    def on_phase_end(self): pass\n",
        "    def on_batch_end(self, loss):\n",
        "        self.iteration += 1\n",
        "        self.writer.add_scalar(\"training_loss\", loss, self.iteration)\n",
        "    def on_train_end(self):\n",
        "        self.writer.close()\n",
        "        \n",
        "        \n",
        "class BestModelCheckPoint(Callback):\n",
        "    def __init__(self, learner, path, model_name, lr):\n",
        "        super().__init__()\n",
        "        self.learner = learner\n",
        "        self.model_name = model_name\n",
        "        self.learning_rate = lr\n",
        "        self.model_log = {}\n",
        "        self.model_path = self.learner.models_path\n",
        "        os.makedirs(self.model_path, exist_ok=True)\n",
        "\n",
        "    def on_train_begin(self): \n",
        "        self.first_epoch = True\n",
        "        self.epoch = 0\n",
        "        self.best_loss = 0.\n",
        "\n",
        "    def on_batch_begin(self): pass\n",
        "    def on_phase_begin(self): pass\n",
        "    def on_epoch_end(self, metrics): \n",
        "        self.epoch += 1\n",
        "        self.val_loss = metrics[0]\n",
        "        if self.first_epoch:\n",
        "            self.best_loss = self.val_loss\n",
        "            self.first_epoch = False\n",
        "        elif self.val_loss < self.best_loss:\n",
        "            self.best_loss = self.val_loss\n",
        "            self.learner.save(self.model_name)\n",
        "            self.model_log['training_loss'] = [str(self.train_losses)]\n",
        "            self.model_log['validation_loss'] = [str(self.val_loss)]\n",
        "            self.model_log['epoch_num'] = [str(self.epoch)]\n",
        "            self.model_log['learning_rate'] = [str(self.learning_rate)]\n",
        "            self.model_log['model_info'] = [w for s in [str(self.learner.model)] for w in s.split('\\n')]\n",
        "            self.model_log['model_info'].append(\"(attention_type): %s\" %self.learner.model.attention_type)\n",
        "            self.model_log['model_info'].append(\"(weight_tie): %s\" %self.learner.model.tied_weight_type)\n",
        "            self.model_log['model_info'].append(\"(pre_trained_vector_type): %s\" %self.learner.model.pre_trained_vector_type)\n",
        "            self.model_log['model_info'].append(\"(teacher_forcing): %s\" %self.learner.model.teacher_forcing)\n",
        "            if self.learner.model.teacher_forcing: self.model_log['model_info'].append(\"(teacher_forcing_prob): %s\" %self.learner.model.force_prob)\n",
        "            with open(f'{self.model_path}/{self.model_name}_model_log.json', 'w') as d: json.dump(self.model_log, d)\n",
        "        else: pass        \n",
        "    def on_phase_end(self): pass\n",
        "    def on_batch_end(self, loss):\n",
        "        self.train_losses = loss\n",
        "    def on_train_end(self): \n",
        "            self.learner.save(f'{self.model_name}_train_end')\n",
        "            self.model_log['training_loss'] = [str(self.train_losses)]\n",
        "            self.model_log['validation_loss'] = [str(self.val_loss)]\n",
        "            self.model_log['epoch_num'] = [str(self.epoch)]\n",
        "            self.model_log['learning_rate'] = [str(self.learning_rate)]\n",
        "            self.model_log['model_info'] = [w for s in [str(self.learner.model)] for w in s.split('\\n')]\n",
        "            self.model_log['model_info'].append(\"(attention_type): %s\" %self.learner.model.attention_type)\n",
        "            self.model_log['model_info'].append(\"(weight_tie): %s\" %self.learner.model.tied_weight_type)\n",
        "            self.model_log['model_info'].append(\"(pre_trained_vector_type): %s\" %self.learner.model.pre_trained_vector_type)\n",
        "            self.model_log['model_info'].append(\"(teacher_forcing): %s\" %self.learner.model.teacher_forcing)\n",
        "            if self.learner.model.teacher_forcing: self.model_log['model_info'].append(\"(teacher_forcing_prob): %s\" %self.learner.model.force_prob)\n",
        "            with open(f'{self.model_path}/{self.model_name}_train_end_model_log.json', 'w') as d: json.dump(self.model_log, d)\n",
        "\n",
        "class TeacherForcingSched(Callback):\n",
        "    def __init__(self, learner, scheduler):\n",
        "        super().__init__()\n",
        "        self.learner = learner\n",
        "        self.scheduler = scheduler\n",
        "        \n",
        "    def on_train_begin(self): \n",
        "        self.learner.model.force_prob = round(self.scheduler.next_val(),1)\n",
        "        \n",
        "    def on_batch_begin(self): pass\n",
        "    def on_phase_begin(self): pass\n",
        "    def on_epoch_end(self, metrics): \n",
        "        self.learner.model.force_prob = round(self.scheduler.next_val(),1)\n",
        "        \n",
        "    def on_phase_end(self): pass\n",
        "    def on_batch_end(self, loss):pass\n",
        "    def on_train_end(self): pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkdR1Nz-aSDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN_Learner(Learner):\n",
        "    def __init__(self, data, models, **kwargs):\n",
        "        super().__init__(data, models, **kwargs)\n",
        "\n",
        "    def _get_crit(self, data): return F.cross_entropy\n",
        "\n",
        "    def save_encoder(self, name): save_model(self.model[0], self.get_model_path(name))\n",
        "\n",
        "    def load_encoder(self, name): load_model(self.model[0], self.get_model_path(name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgCVp7l6abv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class BasicModel():\n",
        "    def __init__(self,model,name='unnamed'): self.model,self.name = model,name\n",
        "    def get_layer_groups(self, do_fc=False): return children(self.model)\n",
        "\n",
        "class SingleModel(BasicModel):\n",
        "    def get_layer_groups(self): return [self.model]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foeYXoTMbQ9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq_loss(input, target):\n",
        "    sl,bs = target.size()\n",
        "    sl_in,bs_in,nc = input.size()\n",
        "    if sl>sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))\n",
        "    input = input[:sl]\n",
        "    return F.cross_entropy(input.view(-1,nc), target.view(-1))#, ignore_index=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1EXCaQwb6ds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cJAZVC4y7OG",
        "colab_type": "code",
        "outputId": "20e2de53-b4c0-4e5f-9b88-d8766ee162b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "attention_type='luong'\n",
        "model_luong = Seq2SeqRNN(rnn_type, input_size, embz_size, hidden_size, batch_size, output_size, max_tgt_len,\n",
        "               attention_type, tied_weight_type, pre_trained_vector, pre_trained_vector_type, padding_idx)\n",
        "\n",
        "print('='*100)\n",
        "print('Model log:')\n",
        "print(model_luong, '\\n')\n",
        "print('- attention_type = {} \\n'.format(model_luong.attention_type))\n",
        "print('- weight_tie = {} \\n'.format(model_luong.tied_weight_type))\n",
        "print('- teacher_forcing = {} \\n '.format(model_luong.teacher_forcing)) \n",
        "print('- pre_trained_embedding = {} \\n'.format(model_luong.pre_trained_vector_type)) \n",
        "print('='*100 + '\\n')\n",
        "\n",
        "if USE_GPU:\n",
        "    model_luong.cuda()\n",
        "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
        "learn_luong = RNN_Learner(model_data, SingleModel(model_luong), opt_fn=opt_fn)\n",
        "learn_luong.crit = seq2seq_loss\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Model log:\n",
            "Seq2SeqRNN(\n",
            "  (pre_trained_vector): Embedding(52221, 200, padding_idx=1)\n",
            "  (encoder_dropout): Dropout(p=0.0)\n",
            "  (encoder_embedding_layer): Embedding(52221, 200, padding_idx=1)\n",
            "  (encoder_rnn): GRU(200, 200, bidirectional=True)\n",
            "  (encoder_vector_layer): Linear(in_features=400, out_features=200, bias=False)\n",
            "  (decoder_dropout): Dropout(p=0.0)\n",
            "  (decoder_embedding_layer): Embedding(52221, 200, padding_idx=1)\n",
            "  (decoder_rnn): GRU(200, 400)\n",
            "  (decoder_output_layer): Linear(in_features=400, out_features=200, bias=False)\n",
            "  (output_layer): Linear(in_features=200, out_features=52221, bias=False)\n",
            "  (encoder_output_layer): Linear(in_features=400, out_features=200, bias=False)\n",
            "  (att_vector_layer): Linear(in_features=400, out_features=200, bias=False)\n",
            ") \n",
            "\n",
            "- attention_type = Luong \n",
            "\n",
            "- weight_tie = three_way \n",
            "\n",
            "- teacher_forcing = True \n",
            " \n",
            "- pre_trained_embedding = glove.6B.200d \n",
            "\n",
            "====================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bu2LXXNd-Du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerOptimizer():\n",
        "    def __init__(self, opt_fn, layer_groups, lrs, wds=None):\n",
        "        if not isinstance(layer_groups, (list,tuple)): layer_groups=[layer_groups]\n",
        "        lrs = listify(lrs, layer_groups)\n",
        "        if wds is None: wds=0.\n",
        "        wds = listify(wds, layer_groups)\n",
        "        self.layer_groups,self.lrs,self.wds = layer_groups,lrs,wds\n",
        "        self.opt = opt_fn(self.opt_params())\n",
        "\n",
        "    def opt_params(self):\n",
        "        assert len(self.layer_groups) == len(self.lrs), f'size mismatch, expected {len(self.layer_groups)} lrs, but got {len(self.lrs)}'\n",
        "        assert len(self.layer_groups) == len(self.wds), f'size mismatch, expected {len(self.layer_groups)} wds, but got {len(self.wds)}'\n",
        "        params = list(zip(self.layer_groups,self.lrs,self.wds))\n",
        "        return [opt_params(*p) for p in params]\n",
        "\n",
        "    @property\n",
        "    def lr(self): return self.lrs[-1]\n",
        "\n",
        "    @property\n",
        "    def mom(self):\n",
        "        if 'betas' in self.opt.param_groups[0]:\n",
        "            return self.opt.param_groups[0]['betas'][0]\n",
        "        else:\n",
        "            return self.opt.param_groups[0]['momentum']\n",
        "\n",
        "    def set_lrs(self, lrs):\n",
        "        lrs = listify(lrs, self.layer_groups)\n",
        "        set_lrs(self.opt, lrs)\n",
        "        self.lrs=lrs\n",
        "\n",
        "    def set_wds_out(self, wds):\n",
        "        wds = listify(wds, self.layer_groups)\n",
        "        set_wds_out(self.opt, wds)\n",
        "        set_wds(self.opt, [0] * len(self.layer_groups))\n",
        "        self.wds=wds\n",
        "\n",
        "    def set_wds(self, wds):\n",
        "        wds = listify(wds, self.layer_groups)\n",
        "        set_wds(self.opt, wds)\n",
        "        set_wds_out(self.opt, [0] * len(self.layer_groups))\n",
        "        self.wds=wds\n",
        "    \n",
        "    def set_mom(self,momentum):\n",
        "        if 'betas' in self.opt.param_groups[0]:\n",
        "            for pg in self.opt.param_groups: pg['betas'] = (momentum, pg['betas'][1])\n",
        "        else:\n",
        "            for pg in self.opt.param_groups: pg['momentum'] = momentum\n",
        "    \n",
        "    def set_beta(self,beta):\n",
        "        if 'betas' in self.opt.param_groups[0]:\n",
        "            for pg in self.opt.param_groups: pg['betas'] = (pg['betas'][0],beta)\n",
        "        elif 'alpha' in self.opt.param_groups[0]:\n",
        "            for pg in self.opt.param_groups: pg['alpha'] = beta\n",
        "\n",
        "    def set_opt_fn(self, opt_fn):\n",
        "        if type(self.opt) != type(opt_fn(self.opt_params())):\n",
        "            self.opt = opt_fn(self.opt_params())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAf_GOEFeR1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def opt_params(self):\n",
        "        assert len(self.layer_groups) == len(self.lrs), f'size mismatch, expected {len(self.layer_groups)} lrs, but got {len(self.lrs)}'\n",
        "        assert len(self.layer_groups) == len(self.wds), f'size mismatch, expected {len(self.layer_groups)} wds, but got {len(self.wds)}'\n",
        "        params = list(zip(self.layer_groups,self.lrs,self.wds))\n",
        "        return [opt_params(*p) for p in params]\n",
        "    def opt_params(parm, lr, wd):\n",
        "        return {'params': chain_params(parm), 'lr':lr, 'weight_decay':wd}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXhgsCv_eh67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chain_params(p):\n",
        "    if is_listy(p):\n",
        "        return list(chain(*[trainable_params_(o) for o in p]))\n",
        "    return trainable_params_(p)\n",
        "  \n",
        "def is_listy(x): return isinstance(x, (list,tuple))\n",
        "\n",
        "def trainable_params_(m):\n",
        "    '''Returns a list of trainable parameters in the model m. (i.e., those that require gradients.)'''\n",
        "    return [p for p in m.parameters() if p.requires_grad]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr7hGp6zgpyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LossRecorder(Callback):\n",
        "    '''\n",
        "    Saves and displays loss functions and other metrics. \n",
        "    Default sched when none is specified in a learner. \n",
        "    '''\n",
        "    def __init__(self, layer_opt, save_path='', record_mom=False, metrics=[]):\n",
        "        super().__init__()\n",
        "        self.layer_opt=layer_opt\n",
        "        self.init_lrs=np.array(layer_opt.lrs)\n",
        "        self.save_path, self.record_mom, self.metrics = save_path, record_mom, metrics\n",
        "\n",
        "    def on_train_begin(self):\n",
        "        self.losses,self.lrs,self.iterations,self.epochs,self.times = [],[],[],[],[]\n",
        "        self.start_at = timer()\n",
        "        self.val_losses, self.rec_metrics = [], []\n",
        "        if self.record_mom:\n",
        "            self.momentums = []\n",
        "        self.iteration = 0\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, metrics):\n",
        "        self.epoch += 1\n",
        "        self.epochs.append(self.iteration)\n",
        "        self.times.append(timer() - self.start_at)\n",
        "        self.save_metrics(metrics)\n",
        "\n",
        "    def on_batch_end(self, loss):\n",
        "        self.iteration += 1\n",
        "        self.lrs.append(self.layer_opt.lr)\n",
        "        self.iterations.append(self.iteration)\n",
        "        if isinstance(loss, list):\n",
        "            self.losses.append(loss[0])\n",
        "            self.save_metrics(loss[1:])\n",
        "        else: self.losses.append(loss)\n",
        "        if self.record_mom: self.momentums.append(self.layer_opt.mom)\n",
        "\n",
        "    def save_metrics(self,vals):\n",
        "        self.val_losses.append(delistify(vals[0]))\n",
        "        if len(vals) > 2: self.rec_metrics.append(vals[1:])\n",
        "        elif len(vals) == 2: self.rec_metrics.append(vals[1])\n",
        "\n",
        "    def plot_loss(self, n_skip=10, n_skip_end=5):\n",
        "        '''\n",
        "        plots loss function as function of iterations. \n",
        "        When used in Jupyternotebook, plot will be displayed in notebook. Else, plot will be displayed in console and both plot and loss are saved in save_path. \n",
        "        '''\n",
        "        if not in_ipynb(): plt.switch_backend('agg')\n",
        "        plt.plot(self.iterations[n_skip:-n_skip_end], self.losses[n_skip:-n_skip_end])\n",
        "        if not in_ipynb():\n",
        "            plt.savefig(os.path.join(self.save_path, 'loss_plot.png'))\n",
        "            np.save(os.path.join(self.save_path, 'losses.npy'), self.losses[10:])\n",
        "\n",
        "    def plot_lr(self):\n",
        "        '''Plots learning rate in jupyter notebook or console, depending on the enviroment of the learner.'''\n",
        "        if not in_ipynb():\n",
        "            plt.switch_backend('agg')\n",
        "        if self.record_mom:\n",
        "            fig, axs = plt.subplots(1,2,figsize=(12,4))\n",
        "            for i in range(0,2): axs[i].set_xlabel('iterations')\n",
        "            axs[0].set_ylabel('learning rate')\n",
        "            axs[1].set_ylabel('momentum')\n",
        "            axs[0].plot(self.iterations,self.lrs)\n",
        "            axs[1].plot(self.iterations,self.momentums)   \n",
        "        else:\n",
        "            plt.xlabel(\"iterations\")\n",
        "            plt.ylabel(\"learning rate\")\n",
        "            plt.plot(self.iterations, self.lrs)\n",
        "        if not in_ipynb():\n",
        "            plt.savefig(os.path.join(self.save_path, 'lr_plot.png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtn7xXAEfzoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LR_Updater(LossRecorder):\n",
        "    '''\n",
        "    Abstract class where all Learning Rate updaters inherit from. (e.g., CirularLR)\n",
        "    Calculates and updates new learning rate and momentum at the end of each batch. \n",
        "    Have to be extended. \n",
        "    '''\n",
        "    def on_train_begin(self):\n",
        "        super().on_train_begin()\n",
        "        self.update_lr()\n",
        "        if self.record_mom:\n",
        "            self.update_mom()\n",
        "\n",
        "    def on_batch_end(self, loss):\n",
        "        res = super().on_batch_end(loss)\n",
        "        self.update_lr()\n",
        "        if self.record_mom:\n",
        "            self.update_mom()\n",
        "        return res\n",
        "\n",
        "    def update_lr(self):\n",
        "        new_lrs = self.calc_lr(self.init_lrs)\n",
        "        self.layer_opt.set_lrs(new_lrs)\n",
        "    \n",
        "    def update_mom(self):\n",
        "        new_mom = self.calc_mom()\n",
        "        self.layer_opt.set_mom(new_mom)\n",
        "\n",
        "    @abstractmethod\n",
        "    def calc_lr(self, init_lrs): raise NotImplementedError\n",
        "    \n",
        "    @abstractmethod\n",
        "    def calc_mom(self): raise NotImplementedError\n",
        "      \n",
        "class LR_Finder(LR_Updater):\n",
        "    '''\n",
        "    Helps you find an optimal learning rate for a model, as per suggetion of 2015 CLR paper. \n",
        "    Learning rate is increased in linear or log scale, depending on user input, and the result of the loss funciton is retained and can be plotted later. \n",
        "    '''\n",
        "    def __init__(self, layer_opt, nb, end_lr=10, linear=False, metrics = []):\n",
        "        self.linear, self.stop_dv = linear, True\n",
        "        ratio = end_lr/layer_opt.lr\n",
        "        self.lr_mult = (ratio/nb) if linear else ratio**(1/nb)\n",
        "        super().__init__(layer_opt,metrics=metrics)\n",
        "\n",
        "    def on_train_begin(self):\n",
        "        super().on_train_begin()\n",
        "        self.best=1e9\n",
        "\n",
        "    def calc_lr(self, init_lrs):\n",
        "        mult = self.lr_mult*self.iteration if self.linear else self.lr_mult**self.iteration\n",
        "        return init_lrs * mult\n",
        "\n",
        "    def on_batch_end(self, metrics):\n",
        "        loss = metrics[0] if isinstance(metrics,list) else metrics\n",
        "        if self.stop_dv and (math.isnan(loss) or loss>self.best*4):\n",
        "            return True\n",
        "        if (loss<self.best and self.iteration>10): self.best=loss\n",
        "        return super().on_batch_end(metrics)\n",
        "\n",
        "    def plot(self, n_skip=10, n_skip_end=5):\n",
        "        '''\n",
        "        Plots the loss function with respect to learning rate, in log scale. \n",
        "        '''\n",
        "        plt.ylabel(\"validation loss\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(self.lrs[n_skip:-(n_skip_end+1)], self.losses[n_skip:-(n_skip_end+1)])\n",
        "        plt.xscale('log')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv1L1JnWg2ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sum_geom(a,r,n): return a*n if r==1 else math.ceil(a*(1-r**n)/(1-r))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSbicbE_y7OM",
        "colab_type": "code",
        "outputId": "6e5bfb5a-b28c-4f94-9bef-fdf7fee558a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "learn_luong.lr_find()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-f128dc528eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn_luong\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-84eedd0cb133>\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(self, start_lr, end_lr, wds, linear, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR_Finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-84eedd0cb133>\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: fit() got multiple values for argument 'metrics'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_ywZvf3Ky7OQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_luong.sched.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8CS6pRQy7OU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7waXZBNy7OW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Luong Attention model\n",
        "lr=1e-3\n",
        "model_name = f'{model_luong.rnn_type}_{model_luong.attention_type}_rev_{rev}'.lower()\n",
        "cycle_len=15\n",
        "best_model = BestModelCheckPoint(learn_luong, model_data.path, model_name, lr)\n",
        "tb_logger = TensorboardLogger(model_data.path, model_name)\n",
        "sched = DecayScheduler(DecayType.LINEAR, cycle_len, 0.5, 0.1)\n",
        "teach_forcer = TeacherForcingSched(learn_luong, sched)\n",
        "learn_luong.fit(lr, 1, cycle_len=cycle_len, use_clr=(20,10), stepper=Seq2SeqStepper, \\\n",
        "          callbacks=[tb_logger, teach_forcer, best_model])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UIlCp_5y7OZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention_type='bahdanau'\n",
        "model_bahdanau = Seq2SeqRNN(rnn_type, input_size, embz_size, hidden_size, batch_size, output_size, max_tgt_len,\n",
        "               attention_type, tied_weight_type, pre_trained_vector, pre_trained_vector_type, padding_idx)\n",
        "\n",
        "print('='*100)\n",
        "print('Model log:')\n",
        "print(model_bahdanau, '\\n')\n",
        "print('- attention_type = {} \\n'.format(model_bahdanau.attention_type))\n",
        "print('- weight_tie = {} \\n'.format(model_bahdanau.tied_weight_type))\n",
        "print('- teacher_forcing = {} \\n '.format(model_bahdanau.teacher_forcing)) \n",
        "print('- pre_trained_embedding = {} \\n'.format(model_bahdanau.pre_trained_vector_type)) \n",
        "print('='*100 + '\\n')\n",
        "\n",
        "if USE_GPU:\n",
        "    model_bahdanau.cuda()\n",
        "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
        "learn_luong = RNN_Learner(model_data, SingleModel(model_bahdanau), opt_fn=opt_fn)\n",
        "learn.crit = seq2seq_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "mL2tKNOqy7Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bahdanau Attention Model\n",
        "lr=1e-3\n",
        "model_name = f'{model.rnn_type}_{model.attention_type}_rev_{rev}'.lower()\n",
        "cycle_len=15\n",
        "best_model = BestModelCheckPoint(learn, model_data.path, model_name, lr)\n",
        "tb_logger = TensorboardLogger(model_data.path, model_name)\n",
        "sched = DecayScheduler(DecayType.LINEAR, cycle_len, 0.5, 0.1)\n",
        "teach_forcer = TeacherForcingSched(learn, sched)\n",
        "learn.fit(lr, 1, cycle_len=cycle_len, use_clr=(20,10), stepper=Seq2SeqStepper, \\\n",
        "          callbacks=[tb_logger, teach_forcer, best_model])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmXlswv1y7Om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "c8czqvosy7Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    fontdict = {'fontsize': 14}\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "def generate(x, y, m):\n",
        "    probs = m.model(V(x))\n",
        "    preds, attention, encoder_embedding = to_np(probs[0].max(2)[1]), to_np(probs[1].squeeze(1)), to_np(probs[2])\n",
        "    sentence = ' '.join([index_to_string[o] for o in x[:,0].data.cpu().numpy() if o != 1])\n",
        "    result = ' '.join([index_to_string[o] for o in preds[:,0] if o!=1])\n",
        "    orig = ' '.join([index_to_string[o] for o in y[:,0].data.cpu().numpy() if o != 1])\n",
        "    print('Input: {}'.format(sentence), '\\n')\n",
        "    print('Original summary: {}'.format(orig), '\\n')\n",
        "    print('Predicted summary: {}'.format(result))\n",
        "    attention_plot = attention[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
        "    return preds, attention, encoder_embedding\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zndt-uuwy7Ox",
        "colab_type": "code",
        "outputId": "b7a75705-3d1f-46ce-c256-a94526e3d93e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attention_type='luong'\n",
        "model_luong = Seq2SeqRNN(rnn_type, input_size, embz_size, hidden_size, batch_size, output_size, max_tgt_len,\n",
        "               attention_type, tied_weight_type, pre_trained_vector, pre_trained_vector_type, padding_idx)\n",
        "if USE_GPU:\n",
        "    model_luong.cuda()\n",
        "learn_luong = RNN_Learner(model_data, SingleModel(model_luong))\n",
        "learn_luong.load('gru_luong_rev_1_train_end')\n",
        "\n",
        "attention_type='bahdanau'\n",
        "model_bahdanau = Seq2SeqRNN(rnn_type, input_size, embz_size, hidden_size, batch_size, output_size, max_tgt_len,\n",
        "               attention_type, tied_weight_type, pre_trained_vector, pre_trained_vector_type, padding_idx)\n",
        "if USE_GPU:\n",
        "    model_bahdanau.cuda()\n",
        "learn_bahdanau = RNN_Learner(model_data, SingleModel(model_bahdanau))\n",
        "learn_bahdanau.load('gru_bahdanau_rev_1_train_end')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-3a5d30793a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mUSE_GPU\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_luong\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlearn_luong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN_Learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_luong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mlearn_luong\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gru_luong_rev_1_train_end'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RNN_Learner' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "J79HRQfby7O0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Luong (Global Dot) Attention\n",
        "x,y = next(iter(model_data.trn_dl))\n",
        "for i in range(1):\n",
        "    print(i)\n",
        "    preds, attention, encoder_embedding = generate(x.transpose(1,0)[i].unsqueeze(1), y.transpose(1,0)[i].unsqueeze(1), learn_luong)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "0Exc4Lthy7O4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bahdanau Attention\n",
        "for i in range(1):\n",
        "    print(i)\n",
        "    preds, attention, encoder_embedding = generate(x.transpose(1,0)[i].unsqueeze(1), y.transpose(1,0)[i].unsqueeze(1), learn_bahdanau)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}